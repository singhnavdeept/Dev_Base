
 - ## I. Data Engineering — Core Foundations (Non-Negotiable)

- [ ] 1. [[Data Fundamentals]]
	
	- [x] Structured vs Semi-Structured vs Unstructured data
	    
	- [x] OLTP vs OLAP systems
	    
	- [ ] Row-oriented vs Column-oriented storage
	    
	- [ ] Schema-on-write vs Schema-on-read
	    
	- [ ] Data granularity and aggregation levels
	    

> - [ ] Expect MCQs that test _why_ one model is preferred over another.



- [ ] ### 2. Databases & Storage Systems

	- [ ] Primary key, foreign key, indexing
	    
	- [ ] Normalization (1NF–3NF), denormalization
	    
	- [ ] ACID properties
	    
	
	- [ ] **NoSQL**
	
	- [ ] Key-Value, Document, Column-Family, Graph databases
	    
	- [ ] CAP theorem (Consistency, Availability, Partition Tolerance)
	    
	- [ ] Use-case mapping (e.g., why Cassandra over MySQL)
	    
	
	- [ ] **Data Warehousing**
	
	- [ ] Fact vs Dimension tables
	    
	- [ ] Star vs Snowflake schema
	    
	- [ ] Slowly Changing Dimensions (SCD Types 1–3)
    

- [ ] ---

- [ ] ### 3. Data Ingestion & ETL/ELT

- [ ] ETL vs ELT (and when ELT is superior)
    
- [ ] Batch vs Stream processing
    
- [ ] Idempotency in pipelines
    
- [ ] Data validation and data quality checks
    

- [ ] **Tools (Conceptual Familiarity)**

- [ ] Apache Airflow (DAGs, operators, schedulers)
    
- [ ] Apache Kafka (topics, partitions, producers, consumers)
    
- [ ] Apache Spark (RDD vs DataFrame vs Dataset)
    

- [ ] ---

- [ ] ### 4. Big Data & Distributed Systems

- [ ] Horizontal vs Vertical scaling
    
- [ ] HDFS architecture (NameNode, DataNode)
    
- [ ] Spark execution model
    
- [ ] Fault tolerance in distributed systems
    
- [ ] Shuffle operations and their cost
    

> - [ ] MCQs here often revolve around _performance bottlenecks_.

- [ ] ---

- [ ] ### 5. Cloud & Modern Data Stack (High-Probability Area)

- [ ] Data lakes vs Data warehouses vs Lakehouse
    
- [ ] Object storage (S3-like concepts)
    
- [ ] Serverless data processing
    
- [ ] Managed services vs self-hosted trade-offs
    

- [ ] **Common Names You Should Recognize**

- [ ] Snowflake, BigQuery, Redshift
    
- [ ] Glue, Databricks (at least at a conceptual level)
    

- [ ] ---

- [ ] ### 6. Data Governance & Reliability

- [ ] Data lineage
    
- [ ] Data observability
    
- [ ] Data freshness vs data accuracy
    
- [ ] PII, data masking, access control
    
- [ ] Role-based access (RBAC)
    

- [ ] ---

- [ ] ## II. Generative AI — Practical & Conceptual Literacy

- [ ] ### 1. Machine Learning Basics (Minimal but Essential)

- [ ] Supervised vs Unsupervised vs Reinforcement learning
    
- [ ] Overfitting vs Underfitting
    
- [ ] Bias-variance trade-off
    
- [ ] Train/Validation/Test split
    

- [ ] ---

- [ ] ### 2. Deep Learning Foundations

- [ ] Neural networks (layers, weights, activation functions)
    
- [ ] Backpropagation (conceptual understanding)
    
- [ ] CNN vs RNN vs Transformer (why Transformer dominates GenAI)
    

- [ ] ---

- [ ] ### 3. Generative AI Core Concepts (Very High Yield)

- [ ] What makes a model _generative_
    
- [ ] LLMs vs traditional NLP models
    
- [ ] Tokens, embeddings, attention mechanism
    
- [ ] Autoregressive models
    
- [ ] Pre-training vs Fine-tuning
    

- [ ] ---

- [ ] ### 4. Transformers & LLMs

- [ ] Self-attention (why it scales better than RNNs)
    
- [ ] Encoder vs Decoder architecture
    
- [ ] Positional encoding
    
- [ ] Inference vs training
    

- [ ] **You should recognize**

- [ ] GPT-style models
    
- [ ] BERT vs GPT (masked vs autoregressive)
    
- [ ] Why GPT is used for text generation
    

- [ ] ---

- [ ] ### 5. Prompt Engineering (MCQ Goldmine)

- [ ] Zero-shot vs Few-shot prompting
    
- [ ] Prompt templates
    
- [ ] System vs User vs Assistant roles
    
- [ ] Hallucination (causes and mitigation)
    

- [ ] ---

- [ ] ### 6. Retrieval-Augmented Generation (RAG)

- [ ] Why LLMs need external knowledge
    
- [ ] Vector databases (FAISS, Pinecone — conceptually)
    
- [ ] Embeddings and similarity search
    
- [ ] When RAG is preferred over fine-tuning
    

- [ ] ---

- [ ] ### 7. Model Fine-Tuning & Adaptation

- [ ] Fine-tuning vs Instruction tuning
    
- [ ] LoRA / parameter-efficient fine-tuning (high-level)
    
- [ ] Catastrophic forgetting (conceptual)
    

- [ ] ---

- [ ] ### 8. Ethics, Safety & Limitations

- [ ] Bias in LLMs
    
- [ ] Data leakage
    
- [ ] Prompt injection
    
- [ ] Privacy concerns
    
- [ ] Explainability limitations
    

- [ ] ---

- [ ] ## III. Cross-Cutting Industry Knowledge (Frequently Tested)

- [ ] ### 1. MLOps / LLMOps (Awareness Level)

- [ ] Model versioning
    
- [ ] Model monitoring
    
- [ ] Data drift vs Concept drift
    
- [ ] Offline vs Online inference
    

- [ ] ---

- [ ] ### 2. Real-World Use Cases

- [ ] Chatbots
    
- [ ] Recommendation systems
    
- [ ] Document summarization
    
- [ ] Code generation
    
- [ ] Data analytics copilots
    

- [ ] Expect scenario-based MCQs:

> - [ ] “Which architecture best fits this requirement?”

- [ ] ---

- [ ] ## IV. What You Can Safely De-Prioritize

- [ ] Heavy mathematics (linear algebra proofs)
    
- [ ] Custom model training from scratch
    
- [ ] Research-grade optimization tricks
    
- [ ] Niche libraries or bleeding-edge papers
    

- [ ] ---

- [ ] ## V. Suggested Study Strategy (Efficient & Civilized)

- [ ] Focus on **definitions + trade-offs**
    
- [ ] Always ask: _why this over that?_
    
- [ ] Practice MCQs that are **architecture- or scenario-based**
    
- [ ] For every tool, know:
    
    - [ ] What problem it solves
        
    - [ ] Why alternatives fail in that scenario
        

- [ ] ---

